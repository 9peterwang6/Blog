<!DOCTYPE html>
<html>
	<head>
		<title>《集体智慧编程》中文版笔记</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="../w3.css">
	</head>

	<body>

		<!-- Header -->
		<div class="w3-container w3-black w3-center">
			<h1>《集体智慧编程》中文版笔记</h1>
		</div>

		<!-- Blog Content -->
		<div class="w3-container w3-black">
			<h3>第2章 提供推荐</h3>
			<p>本章的主要内容是讲如何为用户提供推荐。假设你拥有一个像豆瓣一样的点评类网站数据库，上面记录着不同的用户对不同的内容的评分，从中选出某一个用户和他的记录，如何给这位用户推荐他可能喜欢的内容呢？我们可以先从我们的生活经验出发去尝试解决这个问题。在生活中，我们有什么场景是想要寻求推荐的呢？买电脑、买书、看电影、吃饭等等。在这些场景中，我们会问什么人去获得推荐呢？第一类人是专家，在买电脑时我们可能会看各种博主测评，或者直接问自己认为比较懂电脑的亲朋好友同事。第二类人是用过这些产品的普通用户。</p>
			<p>当我们只考虑专家推荐时，我们会习惯于选择自己相信的专家然后听取他们的意见。在我们的问题中，假设我们能够知道数据库中哪些用户是专家用户，要想用这些专家数据提供推荐，我们还需要找出某一个用户最相信哪一位或者哪几位专家的意见。因为一个用户在听取了专家意见购买产品或者服务后，不一定会在网上反馈他的体验。另一方面，一个用户购买了某款产品的决定并不一定是听取了专家的意见。不过我们现在需要强行缩小考虑范围，方便我们探讨如何根据专家意见做推荐，因此我们需要先下一个强假设：用户的购买决定都是基于专家意见，建立出一个初步模型后再逐渐通过修改这个假设条件来优化我们的模型。在一开始，他可能并不相信任何一位专家，他只是随便看了看几个专家的测评就做决定了，如果说他不喜欢他买到的东西，那么他对产品的评分将会比较低，也不会再相信这位专家了。如果他喜欢买到的产品，他对产品的评分会比较高，他也会继续关注这位专家的测评。将这种情况放到我们的数据集里，我们就会发现我们和我们不喜欢的专家在同一样产品的评分上会有明显差异，而我们和我们喜欢的专家在同一样产品的评分上会比较接近。再抽象一点，我们可以用两点间距离来衡量我们对这些专家的信任程度：设某一普通用户和某一专家用户都评价了N个产品，普通用户对这些产品的评分为(x1, x2, ..., xN)，专家用户对应的评分为(y1, y2, ..., yN)。我们可以计算(x1, x2, ..., xN)和(y1, y2, ..., yN)的距离d=sqrt((x1 - y1)**2 + (x2 - y2)**2 + ... + (xN - yN)**2)。</p>
			<p>求出了距离之后，我们发现距离有大有小，距离越小代表信任程度越高。那我们能不能根据距离构造一个信任程度变量来直接衡量信任程度呢？我们可以令信任程度s=1/(1+d)，这样就得到了一个取值在(0,1]的信任程度变量，s越大信任程度越高。</p>
			<p>得到了信任程度后，我们可以挑选出最信任的（亦即s最大的）前k个专家，对该用户没有购买而这k个专家都购买了的每个产品，计算这个产品的推荐得分p=p1 * s1 + p2 * s2 + ... + pk * sk，其中p1, p2, ..., pk表示这些专家对这个产品的评分，s1, s2, ..., sk表示用户对这些专家的信任程度。将这个产品列表按p从高到低排序，就能得出我们给用户提供的推荐产品列表啦。</p>
			<p>如果我们只考虑普通用户的推荐时，我们同样可以采用像我们考虑专家推荐时的方法来给出一个推荐产品列表。因此，我们可以将所有人都视为专家，然后按照前文所述的方法给出一个推荐列表。这样的话，我们在只考虑专家意见时下的强假设，就可以减弱为：用户的购买决定都是基于其他用户的意见。这种方法有一个名字叫做协作型过滤。对于前文中提到的信任程度的计算，我们也可以考虑使用相关系数来代替，或者使用其他计算两点间距离的方式来进行模型性能提升。</p>
			<p>实际上信任程度的概念可以也引申出用户相似度的概念。用户相似度表示两个用户之间品味的相似程度。信任程度越高，购买方式越接近，相似度就越高。我们既然可以计算两个用户之间品味的相似程度，是否页可以计算两个产品之间的相似程度呢？设某一产品甲和另外一个产品乙都有相同的N个人进行了评价（也就是说这N个用户都评价了这两款产品），这N个用户给甲产品的评分为（x1, x2, ..., xN)，对乙产品的评分为(y1, y2, ..., yN)，我们可以计算这两组评分的距离d=sqrt((x1 - y1)**2 + (x2 - y2)**2 + ... + (xN - yN)**2)。距离越近，代表两个产品的相似程度越高。和前面一样，我们可以引入产品相似度的概念s=1/(1+d)。s越大两个产品相似程度越高。给定某一产品，我们就可以根据s选出k个与该产品最相似的其他产品用作推荐。我们可以用这种方式寻找相似的音乐，同类的书籍和电影等。我们也可以基于物品的相似程度以及用户的评分记录来给用户提供推荐。某个产品的推荐得分p=p1 * s1 + p2 * s2 + ... + pM * sM，其中M为我们评价的产品个数。对于某个产品以及某个用户，我们可以参考它和这个这个用户以往评价过的每个产品的相似度，通过计算加权平均的方式估算出一个可能的评分，将这些产品按照估算出的评分从高到低排列就可一得出我们给用户提供的推荐产品列表。</p>
			<p>在实际应用中，随着用户数量的增加，计算信任程度或用户相似度的开销将会越来越大，而且需要实时更新，这是因为我们总是要及时找到某个用户信任的用户是哪些人，而且我们需要在不同的用户上线的时候就要知道来给他们提供推荐，用户的评价记录更新较快，时常会出现新的信任程度高的用户，因此更新的频率较高。相对而言，产品相似度的结果会随着用户的增多而变得越来越稳定，因此我们只需要每隔一段长时间更新一次就可以得到一个较为准确且稳定的相似程度列表。这两种推荐方法在不同类型的产品中的效果会不一样，我们可以做一个AB测试来判断哪一个方式来判断哪一种方法更受用户喜欢。</p>
			<p>还有一种基于用户记录的推荐方法，是利用条件概率公式计算出在购买某一样产品甲后，会接着购买下一样产品乙的概率。这样在已知用户购买了产品甲后，可以给用户推荐用户大概率会接着购买的几个产品。我们也可以将这种高频率出现的购买组合打包成一个套餐，通过一个优惠的价格吸引更多的用户来购买这些产品。</p>
		</div><br><br>

		<div class="w3-container w3-black">
			<h3>第3章 发现群组</h3>
			<p>本章学习如何构建聚类。构建聚类是一种无监督学习，所谓无监督学习，就是在不带有标准答案的数据中寻找特征的技术。本书中列举了下面几个例子来展示不同的构建聚类以及将聚类可视化的方法。</p>
			<p>第一个例子是构建博客聚类。我们通过简单的浏览，甚至仅仅阅读标题就可以了解一篇博客的主题，这是因为博客的正文或标题中出现了一些我们能够轻易进行分类的关键词。如果我们能够提取出一篇博客的关键词，我们就可以根据关键词的内容对博客进行分类。所以我们要解决的问题是如何提取出博客中的关键词。这个问题要分两步来完成，第一步是提取出博客中的所有词语，第二步是在这些词语中找出可能的关键词。</p>
			<p>如果这篇博客是英语博客，或者其他单词间有分隔的语言的博客，提取词语就非常容易，我们只需要将每个单词提取出来就可以了。但如果这篇博客的语言并没有这种将单词分隔的特性（比如中文、日语）的话，我们就需要借助一些可以完成分词工作的函数库来提取出句子中的所有有意义的词汇。提取出所有的词汇后，我们可以构建一个词汇集合，以便进一步提取出关键词。在这个集合中，我们应该容易发现集合中存在非常多的很普通的形容词、冠词、代词、或者日常用语，看到这些词语的时候，我们根本没有办法判断文章的内容，因此我们需要删去这些词语。要在词汇集合中剔除这些词语，我们可以先构建一个常用词语集合，然后求出我们的词汇集合和常用词语集合的差集即可。另一个方法是我们直接统计词汇集合中，每一个词语在博客中的出现次数。我们知道，许多高频率出现的词汇都是常用词语，因此我们直接根据每个词语出现的次数，删去出现最多的前A%，剩下的词语我们就可以将他们加入可能的关键词集合中。</p>
			<p>对于每一篇博客，我们都可以按照上述的方法构建出一个可能的关键词集合。对每一个这样的关键词集合中的每一个元素，我们可以统计它在博客中出现的次数，然后将出现次数最少的前B%的词语从关键词集合中剔除。这样做是因为博客中可能会出现一些非常生僻的词语，这些词语非常少见，但我们构建聚类时需要考虑这些生僻词语出现次数，于是这些词语也可以像其他出现次数比他们更多的词语一样影响最后聚类的结果，让最后的结果并不能真的很好地将博客分类，所以我们需要将这些词语聪关键词集合中剔除。</p>
			<p>因此，现在我们对收集到的每一篇博客，都可以按照前述的操作得到一个关键词集合，也能够知道关键词集合中每一个词语在该博客中的出现次数。现在我们需要利用这些信息对博客进行聚类。首先我们对这些关键词集合求并集，这样就得到了一个“关键词语表”。利用这个词语表，我们就可以用同一个维度的坐标系表示每一篇博客中的关键词们的出现次数。如果词语表中的某个词并未出现在一篇博客中，该维度的数值为0，若该词语出现在了博客中，则该维度的数值为出现的次数。这样，对每一篇博客i，我们都能得到一个坐标(x_i1, x_i2, ..., x_iN)，其中N表示关键词语表中词语的总数。对于这些坐标，我们就可以像第2章一样，计算任意两个坐标之间的距离。但是和第2章的情况不同，这里我们应该使用（1-相关系数）而不是欧几里得距离来计算距离。这是因为某些博客的长度很长，导致同样的关键词语的出现次数比较多，如果用欧几里得距离来比较的话，与同样主题但是篇幅更短的文章的相似度也可能会比较低，而相关系数则没有这个问题。这些坐标之间的相似度要保存下来，因为在构建聚类的过程中将会被反复用到。</p>
			<p>有了相似度后，有两种方式构建聚类：分级聚类和K均值聚类。分级聚类的原理如下：</p>
			<ol>
				<li>找出相似度最大的两个坐标，然后将他们合并，合并后的坐标变为两个坐标的中点</li>
				<li>计算新的坐标与其他坐标的相似度</li>
				<li>重复第1步只到只有1个坐标</li>
			</ol>
			<p>分级聚类的原理非常像Kruskal算法，我们也可以用树状图的形式将分级聚类的结果呈现出来。</p>
			<p>K均值聚类的原理如下：</p>
			<ol>
				<li>随机生成K个中心点</li>
				<li>计算每个坐标到K个中心点的距离，若某个坐标i距离中心点j的距离最近，则赋予坐标i一个标记j</li>
				<li>对于每个标记j，计算被标记为j的所有坐标的重心。这个重心成为新的中心点j</li>
				<li>重复第2步直到没有中心点发生改变为止</li>
			</ol>
			<p>K均值聚类的结果可以通过一个叫多维缩放的方式，在平面上呈现出来，方法如下：</p>
			<ol>
				<li>使用博客两两间的相似度作为它们之间的目标距离，初始化总误差率为正无穷大</li>
				<li>对于每一篇博客，在平面上随机生成一个坐标</li>
				<li>对于这些坐标，求出两两间当前的欧几里得距离</li>
				<li>将当前距离与目标距离进行比较，求出误差=当前距离 - 目标距离， 将求得的每个误差的绝对值相加，得到新的总误差</li>
				<li>新的总误差小于旧的总误差，进行步骤6和7直到总误差几乎不变为止，否则结束程序</li>
				<li>根据每个求出的误差，按比例移动坐标。比如，点A(x_A, y_A)到点B(x_B, y_B)的当前距离d具有误差e，我们就可以按照如下方法移动点A： x_A = x_A + r * (x_B - x_A) * e / d; y_A = y_A + r * (y_B - y_A) * e / d，其中r大于0表示前进幅度。前进幅度的大小决定循环的次数和效果，前进幅度太小效果好但是循环次数很多，前进幅度太大效果差但循环次数少，r一般取0.01但可以根据实际情况再进行调整。移动公式的推导可以用相似三角形来理解。</li>
				<li>对于新的坐标，重新计算两两间当前的欧几里得距离，跳转到步骤4</li>
			</ol>
			<p>类似于第2章，我们除了可以构造聚类来找出相似的博客以外，还可以通过聚类找出相似的关键词。对于每一个关键词，我们都可以得到一个坐标（x_i1, x_i2, ..., x_iM）表示这个词在每一篇博客中出现的次数，M表示博客的篇数。对于这些坐标，我们可以和前文一样计算任意两个坐标的相似程度s，并使用分级聚类或K均值聚类来构造聚类。</p>
			<p>如果出现的次数只会是0或者1（也就是没有出现或者有出现）的话，我们也可以使用雅卡尔距离=1-对应坐标同为1或同为0的维度数目/(M+M-对应坐标同为1或同为0的维度数目)来作为距离的计算公式。这个情况常见于偏好数据（比如用户喜欢什么讨厌什么，用户已经拥有什么想要拥有什么）中。</p>
		</div><br><br>

		<!-- Footer -->
		<div class="w3-container w3-black w3-center">
			<p> you can contact me via e-mail</p>
		</div>

	</body>
</html>